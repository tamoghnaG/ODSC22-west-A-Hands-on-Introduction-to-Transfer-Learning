{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import remove_empty_docs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets : Amazon Review and IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Amazon Product review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = Loader.load_amazon_reviews('train')\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very diverse dataset and we are taking a subset of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    100020\n",
       "0     99980\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_size = 200000\n",
    "dataset = train_df.sample(n=Sample_size, random_state=42)\n",
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,) (200000,)\n"
     ]
    }
   ],
   "source": [
    "corpus_amazon = dataset['review'].values\n",
    "target_amazon = dataset['sentiment'].values\n",
    "print(corpus_amazon.shape, target_amazon.shape)\n",
    "corpus_amazon, target_amazon = remove_empty_docs(corpus_amazon, target_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading IMDB datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imdb = Loader.load_imdb_data('train')\n",
    "test_df_imdb = Loader.load_imdb_data('test')\n",
    "corpus_imdb = train_df_imdb['review'].values\n",
    "target_imdb = train_df_imdb['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN COUPUS= 125000\n"
     ]
    }
   ],
   "source": [
    "corpus = np.concatenate((corpus_imdb , corpus_amazon))\n",
    "target = np.concatenate((target_imdb , target_amazon))\n",
    "print(\"LEN COUPUS=\",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for Skip-Gram Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May take 2-3 minutes to run\n",
    "w2v_model = Word2Vec(tokenized_corpus,\n",
    "                     sg=1, #FOR SKIP-GRAM\n",
    "                     vector_size = 50,\n",
    "                     window = 5,\n",
    "                     min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulory Size: 44600\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.save_word2vec_format(fname = 'word2vec.txt')\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "vocab_size = len(word_vectors.index_to_key)\n",
    "print(\"Vocabulory Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>broken</th>\n",
       "      <td>shattered</td>\n",
       "      <td>cracked</td>\n",
       "      <td>dented</td>\n",
       "      <td>chipped</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damaged</th>\n",
       "      <td>defective</td>\n",
       "      <td>broken</td>\n",
       "      <td>dented</td>\n",
       "      <td>faulty</td>\n",
       "      <td>scratched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>awsome</td>\n",
       "      <td>amazing</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>excelent</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>usefull</td>\n",
       "      <td>helpful</td>\n",
       "      <td>valuable</td>\n",
       "      <td>beneficial</td>\n",
       "      <td>specific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>great</td>\n",
       "      <td>decent</td>\n",
       "      <td>bad</td>\n",
       "      <td>nice</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>difficult</td>\n",
       "      <td>simple</td>\n",
       "      <td>quick</td>\n",
       "      <td>convenient</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violent</th>\n",
       "      <td>brutal</td>\n",
       "      <td>misogynistic</td>\n",
       "      <td>sadistic</td>\n",
       "      <td>gory</td>\n",
       "      <td>vicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romantic</th>\n",
       "      <td>romance</td>\n",
       "      <td>fairytale</td>\n",
       "      <td>comedy</td>\n",
       "      <td>screwball</td>\n",
       "      <td>erotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>freaky</td>\n",
       "      <td>boob</td>\n",
       "      <td>decapitation</td>\n",
       "      <td>sickening</td>\n",
       "      <td>gross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunate</th>\n",
       "      <td>obvious</td>\n",
       "      <td>chiefly</td>\n",
       "      <td>dubious</td>\n",
       "      <td>redemptive</td>\n",
       "      <td>loathed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictable</th>\n",
       "      <td>implausible</td>\n",
       "      <td>formulaic</td>\n",
       "      <td>clichéd</td>\n",
       "      <td>contrived</td>\n",
       "      <td>talky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hilarious</th>\n",
       "      <td>funny</td>\n",
       "      <td>hysterical</td>\n",
       "      <td>amusing</td>\n",
       "      <td>hillarious</td>\n",
       "      <td>charming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fascinating</th>\n",
       "      <td>compelling</td>\n",
       "      <td>intriguing</td>\n",
       "      <td>facinating</td>\n",
       "      <td>enthralling</td>\n",
       "      <td>recounted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>dull</td>\n",
       "      <td>pointless</td>\n",
       "      <td>tedious</td>\n",
       "      <td>predictable</td>\n",
       "      <td>repetitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confused</th>\n",
       "      <td>annoyed</td>\n",
       "      <td>bored</td>\n",
       "      <td>disturbed</td>\n",
       "      <td>puzzled</td>\n",
       "      <td>distracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensitive</th>\n",
       "      <td>gentle</td>\n",
       "      <td>delicate</td>\n",
       "      <td>abrasive</td>\n",
       "      <td>strenuous</td>\n",
       "      <td>oily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imaginative</th>\n",
       "      <td>inventive</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>engaging</td>\n",
       "      <td>exhilarating</td>\n",
       "      <td>compelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senseless</th>\n",
       "      <td>pointless</td>\n",
       "      <td>absurd</td>\n",
       "      <td>infantile</td>\n",
       "      <td>nonsensical</td>\n",
       "      <td>ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>formulaic</td>\n",
       "      <td>unoriginal</td>\n",
       "      <td>dull</td>\n",
       "      <td>uninspired</td>\n",
       "      <td>lifeless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointing</th>\n",
       "      <td>dissapointing</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>disapointed</td>\n",
       "      <td>dissappointed</td>\n",
       "      <td>disapointing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0             1             2              3  \\\n",
       "broken             shattered       cracked        dented        chipped   \n",
       "damaged            defective        broken        dented         faulty   \n",
       "awesome               awsome       amazing     fantastic       excelent   \n",
       "useful               usefull       helpful      valuable     beneficial   \n",
       "good                   great        decent           bad           nice   \n",
       "easy               difficult        simple         quick     convenient   \n",
       "violent               brutal  misogynistic      sadistic           gory   \n",
       "romantic             romance     fairytale        comedy      screwball   \n",
       "nasty                 freaky          boob  decapitation      sickening   \n",
       "unfortunate          obvious       chiefly       dubious     redemptive   \n",
       "predictable      implausible     formulaic       clichéd      contrived   \n",
       "hilarious              funny    hysterical       amusing     hillarious   \n",
       "fascinating       compelling    intriguing    facinating    enthralling   \n",
       "boring                  dull     pointless       tedious    predictable   \n",
       "confused             annoyed         bored     disturbed        puzzled   \n",
       "sensitive             gentle      delicate      abrasive      strenuous   \n",
       "imaginative        inventive  entertaining      engaging   exhilarating   \n",
       "senseless          pointless        absurd     infantile    nonsensical   \n",
       "bland              formulaic    unoriginal          dull     uninspired   \n",
       "disappointing  dissapointing  disappointed   disapointed  dissappointed   \n",
       "\n",
       "                          4  \n",
       "broken              damaged  \n",
       "damaged           scratched  \n",
       "awesome               great  \n",
       "useful             specific  \n",
       "good                     ok  \n",
       "easy                   hard  \n",
       "violent             vicious  \n",
       "romantic             erotic  \n",
       "nasty                 gross  \n",
       "unfortunate         loathed  \n",
       "predictable           talky  \n",
       "hilarious          charming  \n",
       "fascinating       recounted  \n",
       "boring           repetitive  \n",
       "confused         distracted  \n",
       "sensitive              oily  \n",
       "imaginative      compelling  \n",
       "senseless        ridiculous  \n",
       "bland              lifeless  \n",
       "disappointing  disapointing  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [item[0] for item in word_vectors.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['broken','damaged','awesome','useful','good','easy','violent', 'romantic', 'nasty', 'unfortunate', \n",
    "                                      'predictable', 'hilarious', 'fascinating', 'boring','confused', 'sensitive',\n",
    "                                     'imaginative','senseless', 'bland','disappointing']}\n",
    "pd.DataFrame(similar_words).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentiment Analyser on the Amazon Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from cnn_docmodel import DocClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size = 185000 , test Size = 15000\n"
     ]
    }
   ],
   "source": [
    "train_corpus_amazon = corpus_amazon[:185000]\n",
    "train_target_amazon = target_amazon[:185000]\n",
    "\n",
    "test_corpus_amazon = corpus_amazon[185000:]\n",
    "test_target_amazon = target_amazon[185000:]\n",
    "\n",
    "print(\"Train Size = {} , test Size = {}\".format(len(train_corpus_amazon)\n",
    "                                                , len(test_corpus_amazon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_corpus = [wpt.tokenize(doc.lower()) for doc in train_corpus_amazon]\n",
    "tokenized_test_corpus = [wpt.tokenize(doc.lower()) for doc in test_corpus_amazon] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using zero padding to make document of same size. \n",
    "Also for OOV(out of vocabulary words) we will use index vocab_size+1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordIndex(word):\n",
    "    try :\n",
    "        return word_vectors.get_index(word) + 1\n",
    "    except:\n",
    "        return vocab_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_to_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_train_corpus]\n",
    "MAX_TR_SEQ_LEN = int(np.mean([len(seq) for seq in corpus_to_seq]))\n",
    "corpus_to_seq = pad_sequences(corpus_to_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6119,   98,  593,    1,   75,   25,    2, 4220,  972,    3,    2,\n",
       "       6119,   11,   98,  593,   15,   30, 4156,    1,   99,    3,    2,\n",
       "         25,  442, 2398, 3918,   37,   20,   27,    6, 9647,   10,    6,\n",
       "        821, 2781, 7652,    1,   23,    2,  206,    7,  560,   43, 5515,\n",
       "          1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_to_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_amazon_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_test_corpus]\n",
    "test_corpus_amazon_seq = pad_sequences(test_corpus_amazon_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CNN Document Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cnn_document_model (Documen  multiple                 2260881   \n",
      " tEncoder)                                                       \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,261,138\n",
      "Trainable params: 2,261,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = DocClassifier(vocab_size+2, #0 for null tok & for OOV - vocab_size+1\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier(np.array([np.arange(50)]))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_document_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     multiple                  2230100   \n",
      "                                                                 \n",
      " conv1d_27 (Conv1D)          multiple                  2525      \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          multiple                  3775      \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          multiple                  5025      \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_18 (Dense)            multiple                  19456     \n",
      "                                                                 \n",
      " global_max_pooling1d_9 (Glo  multiple                 0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260,881\n",
      "Trainable params: 2,260,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.doc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inilialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.concatenate([np.zeros((1,50)),\n",
    "                             word_vectors.vectors,\n",
    "                             np.expand_dims(np.mean(word_vectors.vectors, axis=0), axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.doc_model.embedding.set_weights([embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate SA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "#mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.7437 Acc=0.4375\n",
      "epoch 0 step 100 Loss=0.5841 Acc=0.6847\n",
      "epoch 0 step 200 Loss=0.4791 Acc=0.7617\n",
      "epoch 0 step 300 Loss=0.4258 Acc=0.7930\n",
      "epoch 0 step 400 Loss=0.3960 Acc=0.8113\n",
      "epoch 0 step 500 Loss=0.3748 Acc=0.8243\n",
      "epoch 0 step 600 Loss=0.3595 Acc=0.8341\n",
      "epoch 0 step 700 Loss=0.3480 Acc=0.8406\n",
      "epoch 0 step 800 Loss=0.3384 Acc=0.8464\n",
      "epoch 0 step 900 Loss=0.3331 Acc=0.8501\n",
      "epoch 0 step 1000 Loss=0.3258 Acc=0.8546\n",
      "epoch 0 step 1100 Loss=0.3197 Acc=0.8582\n",
      "epoch 0 step 1200 Loss=0.3147 Acc=0.8608\n",
      "epoch 0 step 1300 Loss=0.3096 Acc=0.8635\n",
      "epoch 0 step 1400 Loss=0.3041 Acc=0.8663\n",
      "epoch 0 step 1500 Loss=0.3000 Acc=0.8686\n",
      "epoch 0 step 1600 Loss=0.2966 Acc=0.8706\n",
      "epoch 0 step 1700 Loss=0.2935 Acc=0.8725\n",
      "epoch 0 step 1800 Loss=0.2910 Acc=0.8739\n",
      "epoch 0 step 1900 Loss=0.2881 Acc=0.8755\n",
      "epoch 0 step 2000 Loss=0.2862 Acc=0.8766\n",
      "epoch 0 step 2100 Loss=0.2837 Acc=0.8780\n",
      "epoch 0 step 2200 Loss=0.2812 Acc=0.8793\n",
      "epoch 0 step 2300 Loss=0.2789 Acc=0.8806\n",
      "epoch 0 step 2400 Loss=0.2767 Acc=0.8817\n",
      "epoch 0 step 2500 Loss=0.2747 Acc=0.8826\n",
      "epoch 0 step 2600 Loss=0.2727 Acc=0.8837\n",
      "epoch 0 step 2700 Loss=0.2714 Acc=0.8845\n",
      "epoch 0 step 2800 Loss=0.2699 Acc=0.8852\n",
      "Validation Loss = 0.2235 Acc=0.9122\n",
      "epoch 1 step 0 Loss=0.1424 Acc=0.9375\n",
      "epoch 1 step 100 Loss=0.2217 Acc=0.9066\n",
      "epoch 1 step 200 Loss=0.2173 Acc=0.9113\n",
      "epoch 1 step 300 Loss=0.2093 Acc=0.9155\n",
      "epoch 1 step 400 Loss=0.2025 Acc=0.9188\n",
      "epoch 1 step 500 Loss=0.2007 Acc=0.9203\n",
      "epoch 1 step 600 Loss=0.1984 Acc=0.9215\n",
      "epoch 1 step 700 Loss=0.1970 Acc=0.9227\n",
      "epoch 1 step 800 Loss=0.1947 Acc=0.9237\n",
      "epoch 1 step 900 Loss=0.1943 Acc=0.9242\n",
      "epoch 1 step 1000 Loss=0.1942 Acc=0.9239\n",
      "epoch 1 step 1100 Loss=0.1931 Acc=0.9242\n",
      "epoch 1 step 1200 Loss=0.1914 Acc=0.9248\n",
      "epoch 1 step 1300 Loss=0.1909 Acc=0.9254\n",
      "epoch 1 step 1400 Loss=0.1889 Acc=0.9262\n",
      "epoch 1 step 1500 Loss=0.1876 Acc=0.9268\n",
      "epoch 1 step 1600 Loss=0.1868 Acc=0.9272\n",
      "epoch 1 step 1700 Loss=0.1861 Acc=0.9278\n",
      "epoch 1 step 1800 Loss=0.1856 Acc=0.9280\n",
      "epoch 1 step 1900 Loss=0.1856 Acc=0.9280\n",
      "epoch 1 step 2000 Loss=0.1855 Acc=0.9280\n",
      "epoch 1 step 2100 Loss=0.1849 Acc=0.9283\n",
      "epoch 1 step 2200 Loss=0.1839 Acc=0.9287\n",
      "epoch 1 step 2300 Loss=0.1832 Acc=0.9289\n",
      "epoch 1 step 2400 Loss=0.1825 Acc=0.9292\n",
      "epoch 1 step 2500 Loss=0.1816 Acc=0.9297\n",
      "epoch 1 step 2600 Loss=0.1811 Acc=0.9300\n",
      "epoch 1 step 2700 Loss=0.1808 Acc=0.9302\n",
      "epoch 1 step 2800 Loss=0.1809 Acc=0.9302\n",
      "Validation Loss = 0.2228 Acc=0.9143\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_to_seq,train_target_amazon))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_amazon_seq,test_target_amazon))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        gradients = tape.gradient(loss, classifier.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, classifier.trainable_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "            \n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_iterator) :\n",
    "        test_pred =  classifier(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Validation Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_weights('./model/sa/sentiment_analyser')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on IMDB test w/o Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_imdb = test_df_imdb['review'].tolist()\n",
    "test_target_imdb = test_df_imdb['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_corpus_imdb = [wpt.tokenize(doc.lower()) for doc in test_corpus_imdb]\n",
    "test_corpus_imdb_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_test_corpus_imdb]\n",
    "test_corpus_imdb_seq = pad_sequences(test_corpus_imdb_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_imdb_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "test_dataset_imdb_iterator = test_dataset_imdb_iterator.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x142621a93c8>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_imdb = DocClassifier(vocab_size+2,\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier_imdb(np.array([np.arange(50)]))\n",
    "#inilialize    \n",
    "classifier_imdb.load_weights('./model/sa/sentiment_analyser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.4682 Acc=0.8088\n"
     ]
    }
   ],
   "source": [
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "    test_pred =  classifier_imdb(test_docs)\n",
    "    testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "    test_loss(testloss)\n",
    "    test_acc.update_state(test_targets, test_pred)\n",
    "print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imdb_small = train_df_imdb.sample(frac=0.30, random_state = 42)\n",
    "corpus_imdb_small = train_df_imdb_small['review'].tolist()\n",
    "target_imdb_small = train_df_imdb_small['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb_small_train_corpus = [wpt.tokenize(doc.lower()) for doc in corpus_imdb_small]\n",
    "corpus_imdb_small_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_imdb_small_train_corpus]\n",
    "corpus_imdb_small_seq = pad_sequences(corpus_imdb_small_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.3464 Acc=0.7812\n",
      "epoch 0 step 100 Loss=0.4623 Acc=0.8091\n",
      "epoch 0 step 200 Loss=0.4392 Acc=0.8111\n",
      "Test Loss = 0.4212 Acc=0.8092\n",
      "epoch 1 step 0 Loss=0.2490 Acc=0.8438\n",
      "epoch 1 step 100 Loss=0.4244 Acc=0.8082\n",
      "epoch 1 step 200 Loss=0.4109 Acc=0.8123\n",
      "Test Loss = 0.4132 Acc=0.8089\n",
      "epoch 2 step 0 Loss=0.3421 Acc=0.9062\n",
      "epoch 2 step 100 Loss=0.4115 Acc=0.8122\n",
      "epoch 2 step 200 Loss=0.4072 Acc=0.8133\n",
      "Test Loss = 0.4113 Acc=0.8104\n",
      "epoch 3 step 0 Loss=0.4801 Acc=0.7812\n",
      "epoch 3 step 100 Loss=0.4120 Acc=0.8134\n",
      "epoch 3 step 200 Loss=0.4024 Acc=0.8178\n",
      "Test Loss = 0.4103 Acc=0.8105\n",
      "epoch 4 step 0 Loss=0.4198 Acc=0.7812\n",
      "epoch 4 step 100 Loss=0.4148 Acc=0.8131\n",
      "epoch 4 step 200 Loss=0.4021 Acc=0.8165\n",
      "Test Loss = 0.4094 Acc=0.8108\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_imdb_small_seq,target_imdb_small))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier_imdb(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        final_layer_weights = classifier_imdb.fc1.trainable_weights + classifier_imdb.doc_model.doc_embedding.trainable_weights\n",
    "        gradients = tape.gradient(loss, final_layer_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, final_layer_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "        test_pred =  classifier_imdb(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on IMDB From Scrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_imdb = DocClassifier(vocab_size+2,\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier_imdb(np.array([np.arange(50)]))\n",
    "#inilialize embeddings   \n",
    "classifier_imdb.doc_model.embedding.set_weights([embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.7338 Acc=0.4688\n",
      "epoch 0 step 100 Loss=0.6922 Acc=0.5186\n",
      "epoch 0 step 200 Loss=0.6893 Acc=0.5339\n",
      "Test Loss = 0.6826 Acc=0.5760\n",
      "epoch 1 step 0 Loss=0.6673 Acc=0.6562\n",
      "epoch 1 step 100 Loss=0.6786 Acc=0.5959\n",
      "epoch 1 step 200 Loss=0.6773 Acc=0.5920\n",
      "Test Loss = 0.6750 Acc=0.6088\n",
      "epoch 2 step 0 Loss=0.6724 Acc=0.6562\n",
      "epoch 2 step 100 Loss=0.6712 Acc=0.6126\n",
      "epoch 2 step 200 Loss=0.6698 Acc=0.6132\n",
      "Test Loss = 0.6754 Acc=0.5689\n",
      "epoch 3 step 0 Loss=0.6547 Acc=0.5625\n",
      "epoch 3 step 100 Loss=0.6657 Acc=0.6132\n",
      "epoch 3 step 200 Loss=0.6634 Acc=0.6155\n",
      "Test Loss = 0.6633 Acc=0.6337\n",
      "epoch 4 step 0 Loss=0.6280 Acc=0.7500\n",
      "epoch 4 step 100 Loss=0.6611 Acc=0.6284\n",
      "epoch 4 step 200 Loss=0.6576 Acc=0.6373\n",
      "Test Loss = 0.6630 Acc=0.6130\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_imdb_small_seq,target_imdb_small))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier_imdb(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        final_layer_weights = classifier_imdb.fc1.trainable_weights + classifier_imdb.doc_model.doc_embedding.trainable_weights\n",
    "        gradients = tape.gradient(loss, final_layer_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, final_layer_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "        test_pred =  classifier_imdb(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
